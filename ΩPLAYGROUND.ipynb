{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[400, 300]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "F.get_image_size(torch.zeros((3, 300, 400)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/scratch/choi/output/Diff-Rep/clip/vit_b_16-laion400m_e32-55e67d44.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positional_embedding\n",
      "text_projection\n",
      "logit_scale\n",
      "visual.class_embedding\n",
      "visual.positional_embedding\n",
      "visual.proj\n",
      "visual.conv1.weight\n",
      "visual.ln_pre.weight\n",
      "visual.ln_pre.bias\n",
      "visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "visual.transformer.resblocks.0.ln_1.weight\n",
      "visual.transformer.resblocks.0.ln_1.bias\n",
      "visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.0.ln_2.weight\n",
      "visual.transformer.resblocks.0.ln_2.bias\n",
      "visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "visual.transformer.resblocks.1.ln_1.weight\n",
      "visual.transformer.resblocks.1.ln_1.bias\n",
      "visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.1.ln_2.weight\n",
      "visual.transformer.resblocks.1.ln_2.bias\n",
      "visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "visual.transformer.resblocks.2.ln_1.weight\n",
      "visual.transformer.resblocks.2.ln_1.bias\n",
      "visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.2.ln_2.weight\n",
      "visual.transformer.resblocks.2.ln_2.bias\n",
      "visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "visual.transformer.resblocks.3.ln_1.weight\n",
      "visual.transformer.resblocks.3.ln_1.bias\n",
      "visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.3.ln_2.weight\n",
      "visual.transformer.resblocks.3.ln_2.bias\n",
      "visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "visual.transformer.resblocks.4.ln_1.weight\n",
      "visual.transformer.resblocks.4.ln_1.bias\n",
      "visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.4.ln_2.weight\n",
      "visual.transformer.resblocks.4.ln_2.bias\n",
      "visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "visual.transformer.resblocks.5.ln_1.weight\n",
      "visual.transformer.resblocks.5.ln_1.bias\n",
      "visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.5.ln_2.weight\n",
      "visual.transformer.resblocks.5.ln_2.bias\n",
      "visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "visual.transformer.resblocks.6.ln_1.weight\n",
      "visual.transformer.resblocks.6.ln_1.bias\n",
      "visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.6.ln_2.weight\n",
      "visual.transformer.resblocks.6.ln_2.bias\n",
      "visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "visual.transformer.resblocks.7.ln_1.weight\n",
      "visual.transformer.resblocks.7.ln_1.bias\n",
      "visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.7.ln_2.weight\n",
      "visual.transformer.resblocks.7.ln_2.bias\n",
      "visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "visual.transformer.resblocks.8.ln_1.weight\n",
      "visual.transformer.resblocks.8.ln_1.bias\n",
      "visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.8.ln_2.weight\n",
      "visual.transformer.resblocks.8.ln_2.bias\n",
      "visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "visual.transformer.resblocks.9.ln_1.weight\n",
      "visual.transformer.resblocks.9.ln_1.bias\n",
      "visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.9.ln_2.weight\n",
      "visual.transformer.resblocks.9.ln_2.bias\n",
      "visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "visual.transformer.resblocks.10.ln_1.weight\n",
      "visual.transformer.resblocks.10.ln_1.bias\n",
      "visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.10.ln_2.weight\n",
      "visual.transformer.resblocks.10.ln_2.bias\n",
      "visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "visual.transformer.resblocks.11.ln_1.weight\n",
      "visual.transformer.resblocks.11.ln_1.bias\n",
      "visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "visual.transformer.resblocks.11.ln_2.weight\n",
      "visual.transformer.resblocks.11.ln_2.bias\n",
      "visual.ln_post.weight\n",
      "visual.ln_post.bias\n",
      "transformer.resblocks.0.attn.in_proj_weight\n",
      "transformer.resblocks.0.attn.in_proj_bias\n",
      "transformer.resblocks.0.attn.out_proj.weight\n",
      "transformer.resblocks.0.attn.out_proj.bias\n",
      "transformer.resblocks.0.ln_1.weight\n",
      "transformer.resblocks.0.ln_1.bias\n",
      "transformer.resblocks.0.mlp.c_fc.weight\n",
      "transformer.resblocks.0.mlp.c_fc.bias\n",
      "transformer.resblocks.0.mlp.c_proj.weight\n",
      "transformer.resblocks.0.mlp.c_proj.bias\n",
      "transformer.resblocks.0.ln_2.weight\n",
      "transformer.resblocks.0.ln_2.bias\n",
      "transformer.resblocks.1.attn.in_proj_weight\n",
      "transformer.resblocks.1.attn.in_proj_bias\n",
      "transformer.resblocks.1.attn.out_proj.weight\n",
      "transformer.resblocks.1.attn.out_proj.bias\n",
      "transformer.resblocks.1.ln_1.weight\n",
      "transformer.resblocks.1.ln_1.bias\n",
      "transformer.resblocks.1.mlp.c_fc.weight\n",
      "transformer.resblocks.1.mlp.c_fc.bias\n",
      "transformer.resblocks.1.mlp.c_proj.weight\n",
      "transformer.resblocks.1.mlp.c_proj.bias\n",
      "transformer.resblocks.1.ln_2.weight\n",
      "transformer.resblocks.1.ln_2.bias\n",
      "transformer.resblocks.2.attn.in_proj_weight\n",
      "transformer.resblocks.2.attn.in_proj_bias\n",
      "transformer.resblocks.2.attn.out_proj.weight\n",
      "transformer.resblocks.2.attn.out_proj.bias\n",
      "transformer.resblocks.2.ln_1.weight\n",
      "transformer.resblocks.2.ln_1.bias\n",
      "transformer.resblocks.2.mlp.c_fc.weight\n",
      "transformer.resblocks.2.mlp.c_fc.bias\n",
      "transformer.resblocks.2.mlp.c_proj.weight\n",
      "transformer.resblocks.2.mlp.c_proj.bias\n",
      "transformer.resblocks.2.ln_2.weight\n",
      "transformer.resblocks.2.ln_2.bias\n",
      "transformer.resblocks.3.attn.in_proj_weight\n",
      "transformer.resblocks.3.attn.in_proj_bias\n",
      "transformer.resblocks.3.attn.out_proj.weight\n",
      "transformer.resblocks.3.attn.out_proj.bias\n",
      "transformer.resblocks.3.ln_1.weight\n",
      "transformer.resblocks.3.ln_1.bias\n",
      "transformer.resblocks.3.mlp.c_fc.weight\n",
      "transformer.resblocks.3.mlp.c_fc.bias\n",
      "transformer.resblocks.3.mlp.c_proj.weight\n",
      "transformer.resblocks.3.mlp.c_proj.bias\n",
      "transformer.resblocks.3.ln_2.weight\n",
      "transformer.resblocks.3.ln_2.bias\n",
      "transformer.resblocks.4.attn.in_proj_weight\n",
      "transformer.resblocks.4.attn.in_proj_bias\n",
      "transformer.resblocks.4.attn.out_proj.weight\n",
      "transformer.resblocks.4.attn.out_proj.bias\n",
      "transformer.resblocks.4.ln_1.weight\n",
      "transformer.resblocks.4.ln_1.bias\n",
      "transformer.resblocks.4.mlp.c_fc.weight\n",
      "transformer.resblocks.4.mlp.c_fc.bias\n",
      "transformer.resblocks.4.mlp.c_proj.weight\n",
      "transformer.resblocks.4.mlp.c_proj.bias\n",
      "transformer.resblocks.4.ln_2.weight\n",
      "transformer.resblocks.4.ln_2.bias\n",
      "transformer.resblocks.5.attn.in_proj_weight\n",
      "transformer.resblocks.5.attn.in_proj_bias\n",
      "transformer.resblocks.5.attn.out_proj.weight\n",
      "transformer.resblocks.5.attn.out_proj.bias\n",
      "transformer.resblocks.5.ln_1.weight\n",
      "transformer.resblocks.5.ln_1.bias\n",
      "transformer.resblocks.5.mlp.c_fc.weight\n",
      "transformer.resblocks.5.mlp.c_fc.bias\n",
      "transformer.resblocks.5.mlp.c_proj.weight\n",
      "transformer.resblocks.5.mlp.c_proj.bias\n",
      "transformer.resblocks.5.ln_2.weight\n",
      "transformer.resblocks.5.ln_2.bias\n",
      "transformer.resblocks.6.attn.in_proj_weight\n",
      "transformer.resblocks.6.attn.in_proj_bias\n",
      "transformer.resblocks.6.attn.out_proj.weight\n",
      "transformer.resblocks.6.attn.out_proj.bias\n",
      "transformer.resblocks.6.ln_1.weight\n",
      "transformer.resblocks.6.ln_1.bias\n",
      "transformer.resblocks.6.mlp.c_fc.weight\n",
      "transformer.resblocks.6.mlp.c_fc.bias\n",
      "transformer.resblocks.6.mlp.c_proj.weight\n",
      "transformer.resblocks.6.mlp.c_proj.bias\n",
      "transformer.resblocks.6.ln_2.weight\n",
      "transformer.resblocks.6.ln_2.bias\n",
      "transformer.resblocks.7.attn.in_proj_weight\n",
      "transformer.resblocks.7.attn.in_proj_bias\n",
      "transformer.resblocks.7.attn.out_proj.weight\n",
      "transformer.resblocks.7.attn.out_proj.bias\n",
      "transformer.resblocks.7.ln_1.weight\n",
      "transformer.resblocks.7.ln_1.bias\n",
      "transformer.resblocks.7.mlp.c_fc.weight\n",
      "transformer.resblocks.7.mlp.c_fc.bias\n",
      "transformer.resblocks.7.mlp.c_proj.weight\n",
      "transformer.resblocks.7.mlp.c_proj.bias\n",
      "transformer.resblocks.7.ln_2.weight\n",
      "transformer.resblocks.7.ln_2.bias\n",
      "transformer.resblocks.8.attn.in_proj_weight\n",
      "transformer.resblocks.8.attn.in_proj_bias\n",
      "transformer.resblocks.8.attn.out_proj.weight\n",
      "transformer.resblocks.8.attn.out_proj.bias\n",
      "transformer.resblocks.8.ln_1.weight\n",
      "transformer.resblocks.8.ln_1.bias\n",
      "transformer.resblocks.8.mlp.c_fc.weight\n",
      "transformer.resblocks.8.mlp.c_fc.bias\n",
      "transformer.resblocks.8.mlp.c_proj.weight\n",
      "transformer.resblocks.8.mlp.c_proj.bias\n",
      "transformer.resblocks.8.ln_2.weight\n",
      "transformer.resblocks.8.ln_2.bias\n",
      "transformer.resblocks.9.attn.in_proj_weight\n",
      "transformer.resblocks.9.attn.in_proj_bias\n",
      "transformer.resblocks.9.attn.out_proj.weight\n",
      "transformer.resblocks.9.attn.out_proj.bias\n",
      "transformer.resblocks.9.ln_1.weight\n",
      "transformer.resblocks.9.ln_1.bias\n",
      "transformer.resblocks.9.mlp.c_fc.weight\n",
      "transformer.resblocks.9.mlp.c_fc.bias\n",
      "transformer.resblocks.9.mlp.c_proj.weight\n",
      "transformer.resblocks.9.mlp.c_proj.bias\n",
      "transformer.resblocks.9.ln_2.weight\n",
      "transformer.resblocks.9.ln_2.bias\n",
      "transformer.resblocks.10.attn.in_proj_weight\n",
      "transformer.resblocks.10.attn.in_proj_bias\n",
      "transformer.resblocks.10.attn.out_proj.weight\n",
      "transformer.resblocks.10.attn.out_proj.bias\n",
      "transformer.resblocks.10.ln_1.weight\n",
      "transformer.resblocks.10.ln_1.bias\n",
      "transformer.resblocks.10.mlp.c_fc.weight\n",
      "transformer.resblocks.10.mlp.c_fc.bias\n",
      "transformer.resblocks.10.mlp.c_proj.weight\n",
      "transformer.resblocks.10.mlp.c_proj.bias\n",
      "transformer.resblocks.10.ln_2.weight\n",
      "transformer.resblocks.10.ln_2.bias\n",
      "transformer.resblocks.11.attn.in_proj_weight\n",
      "transformer.resblocks.11.attn.in_proj_bias\n",
      "transformer.resblocks.11.attn.out_proj.weight\n",
      "transformer.resblocks.11.attn.out_proj.bias\n",
      "transformer.resblocks.11.ln_1.weight\n",
      "transformer.resblocks.11.ln_1.bias\n",
      "transformer.resblocks.11.mlp.c_fc.weight\n",
      "transformer.resblocks.11.mlp.c_fc.bias\n",
      "transformer.resblocks.11.mlp.c_proj.weight\n",
      "transformer.resblocks.11.mlp.c_proj.bias\n",
      "transformer.resblocks.11.ln_2.weight\n",
      "transformer.resblocks.11.ln_2.bias\n",
      "token_embedding.weight\n",
      "ln_final.weight\n",
      "ln_final.bias\n"
     ]
    }
   ],
   "source": [
    "for key in checkpoint.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck2 = torch.load(\"/scratch/choi/output/Diff-Rep/mae/mae_pretrain_vit_base.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.11.attn.qkv.bias\n"
     ]
    }
   ],
   "source": [
    "for key in ck2['model'].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl = '/scratch/choi/output/Diff-Rep/clip/pytorch_model.bin?download=true'\n",
    "# with open(pkl,'rb') as f:\n",
    "# o = pickle.load(f)\n",
    "\n",
    "ckpt = torch.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from functools import partial\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(\n",
    "    patch_size=16,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"cls_token\", \"pos_embed\", \"patch_embed.proj.weight\", \"patch_embed.proj.bias\", \"blocks.0.norm1.weight\", \"blocks.0.norm1.bias\", \"blocks.0.attn.qkv.weight\", \"blocks.0.attn.qkv.bias\", \"blocks.0.attn.proj.weight\", \"blocks.0.attn.proj.bias\", \"blocks.0.norm2.weight\", \"blocks.0.norm2.bias\", \"blocks.0.mlp.fc1.weight\", \"blocks.0.mlp.fc1.bias\", \"blocks.0.mlp.fc2.weight\", \"blocks.0.mlp.fc2.bias\", \"blocks.1.norm1.weight\", \"blocks.1.norm1.bias\", \"blocks.1.attn.qkv.weight\", \"blocks.1.attn.qkv.bias\", \"blocks.1.attn.proj.weight\", \"blocks.1.attn.proj.bias\", \"blocks.1.norm2.weight\", \"blocks.1.norm2.bias\", \"blocks.1.mlp.fc1.weight\", \"blocks.1.mlp.fc1.bias\", \"blocks.1.mlp.fc2.weight\", \"blocks.1.mlp.fc2.bias\", \"blocks.2.norm1.weight\", \"blocks.2.norm1.bias\", \"blocks.2.attn.qkv.weight\", \"blocks.2.attn.qkv.bias\", \"blocks.2.attn.proj.weight\", \"blocks.2.attn.proj.bias\", \"blocks.2.norm2.weight\", \"blocks.2.norm2.bias\", \"blocks.2.mlp.fc1.weight\", \"blocks.2.mlp.fc1.bias\", \"blocks.2.mlp.fc2.weight\", \"blocks.2.mlp.fc2.bias\", \"blocks.3.norm1.weight\", \"blocks.3.norm1.bias\", \"blocks.3.attn.qkv.weight\", \"blocks.3.attn.qkv.bias\", \"blocks.3.attn.proj.weight\", \"blocks.3.attn.proj.bias\", \"blocks.3.norm2.weight\", \"blocks.3.norm2.bias\", \"blocks.3.mlp.fc1.weight\", \"blocks.3.mlp.fc1.bias\", \"blocks.3.mlp.fc2.weight\", \"blocks.3.mlp.fc2.bias\", \"blocks.4.norm1.weight\", \"blocks.4.norm1.bias\", \"blocks.4.attn.qkv.weight\", \"blocks.4.attn.qkv.bias\", \"blocks.4.attn.proj.weight\", \"blocks.4.attn.proj.bias\", \"blocks.4.norm2.weight\", \"blocks.4.norm2.bias\", \"blocks.4.mlp.fc1.weight\", \"blocks.4.mlp.fc1.bias\", \"blocks.4.mlp.fc2.weight\", \"blocks.4.mlp.fc2.bias\", \"blocks.5.norm1.weight\", \"blocks.5.norm1.bias\", \"blocks.5.attn.qkv.weight\", \"blocks.5.attn.qkv.bias\", \"blocks.5.attn.proj.weight\", \"blocks.5.attn.proj.bias\", \"blocks.5.norm2.weight\", \"blocks.5.norm2.bias\", \"blocks.5.mlp.fc1.weight\", \"blocks.5.mlp.fc1.bias\", \"blocks.5.mlp.fc2.weight\", \"blocks.5.mlp.fc2.bias\", \"blocks.6.norm1.weight\", \"blocks.6.norm1.bias\", \"blocks.6.attn.qkv.weight\", \"blocks.6.attn.qkv.bias\", \"blocks.6.attn.proj.weight\", \"blocks.6.attn.proj.bias\", \"blocks.6.norm2.weight\", \"blocks.6.norm2.bias\", \"blocks.6.mlp.fc1.weight\", \"blocks.6.mlp.fc1.bias\", \"blocks.6.mlp.fc2.weight\", \"blocks.6.mlp.fc2.bias\", \"blocks.7.norm1.weight\", \"blocks.7.norm1.bias\", \"blocks.7.attn.qkv.weight\", \"blocks.7.attn.qkv.bias\", \"blocks.7.attn.proj.weight\", \"blocks.7.attn.proj.bias\", \"blocks.7.norm2.weight\", \"blocks.7.norm2.bias\", \"blocks.7.mlp.fc1.weight\", \"blocks.7.mlp.fc1.bias\", \"blocks.7.mlp.fc2.weight\", \"blocks.7.mlp.fc2.bias\", \"blocks.8.norm1.weight\", \"blocks.8.norm1.bias\", \"blocks.8.attn.qkv.weight\", \"blocks.8.attn.qkv.bias\", \"blocks.8.attn.proj.weight\", \"blocks.8.attn.proj.bias\", \"blocks.8.norm2.weight\", \"blocks.8.norm2.bias\", \"blocks.8.mlp.fc1.weight\", \"blocks.8.mlp.fc1.bias\", \"blocks.8.mlp.fc2.weight\", \"blocks.8.mlp.fc2.bias\", \"blocks.9.norm1.weight\", \"blocks.9.norm1.bias\", \"blocks.9.attn.qkv.weight\", \"blocks.9.attn.qkv.bias\", \"blocks.9.attn.proj.weight\", \"blocks.9.attn.proj.bias\", \"blocks.9.norm2.weight\", \"blocks.9.norm2.bias\", \"blocks.9.mlp.fc1.weight\", \"blocks.9.mlp.fc1.bias\", \"blocks.9.mlp.fc2.weight\", \"blocks.9.mlp.fc2.bias\", \"blocks.10.norm1.weight\", \"blocks.10.norm1.bias\", \"blocks.10.attn.qkv.weight\", \"blocks.10.attn.qkv.bias\", \"blocks.10.attn.proj.weight\", \"blocks.10.attn.proj.bias\", \"blocks.10.norm2.weight\", \"blocks.10.norm2.bias\", \"blocks.10.mlp.fc1.weight\", \"blocks.10.mlp.fc1.bias\", \"blocks.10.mlp.fc2.weight\", \"blocks.10.mlp.fc2.bias\", \"blocks.11.norm1.weight\", \"blocks.11.norm1.bias\", \"blocks.11.attn.qkv.weight\", \"blocks.11.attn.qkv.bias\", \"blocks.11.attn.proj.weight\", \"blocks.11.attn.proj.bias\", \"blocks.11.norm2.weight\", \"blocks.11.norm2.bias\", \"blocks.11.mlp.fc1.weight\", \"blocks.11.mlp.fc1.bias\", \"blocks.11.mlp.fc2.weight\", \"blocks.11.mlp.fc2.bias\", \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"positional_embedding\", \"text_projection\", \"logit_scale\", \"visual.class_embedding\", \"visual.positional_embedding\", \"visual.proj\", \"visual.conv1.weight\", \"visual.ln_pre.weight\", \"visual.ln_pre.bias\", \"visual.transformer.resblocks.0.ln_1.weight\", \"visual.transformer.resblocks.0.ln_1.bias\", \"visual.transformer.resblocks.0.attn.in_proj_weight\", \"visual.transformer.resblocks.0.attn.in_proj_bias\", \"visual.transformer.resblocks.0.attn.out_proj.weight\", \"visual.transformer.resblocks.0.attn.out_proj.bias\", \"visual.transformer.resblocks.0.ln_2.weight\", \"visual.transformer.resblocks.0.ln_2.bias\", \"visual.transformer.resblocks.0.mlp.c_fc.weight\", \"visual.transformer.resblocks.0.mlp.c_fc.bias\", \"visual.transformer.resblocks.0.mlp.c_proj.weight\", \"visual.transformer.resblocks.0.mlp.c_proj.bias\", \"visual.transformer.resblocks.1.ln_1.weight\", \"visual.transformer.resblocks.1.ln_1.bias\", \"visual.transformer.resblocks.1.attn.in_proj_weight\", \"visual.transformer.resblocks.1.attn.in_proj_bias\", \"visual.transformer.resblocks.1.attn.out_proj.weight\", \"visual.transformer.resblocks.1.attn.out_proj.bias\", \"visual.transformer.resblocks.1.ln_2.weight\", \"visual.transformer.resblocks.1.ln_2.bias\", \"visual.transformer.resblocks.1.mlp.c_fc.weight\", \"visual.transformer.resblocks.1.mlp.c_fc.bias\", \"visual.transformer.resblocks.1.mlp.c_proj.weight\", \"visual.transformer.resblocks.1.mlp.c_proj.bias\", \"visual.transformer.resblocks.2.ln_1.weight\", \"visual.transformer.resblocks.2.ln_1.bias\", \"visual.transformer.resblocks.2.attn.in_proj_weight\", \"visual.transformer.resblocks.2.attn.in_proj_bias\", \"visual.transformer.resblocks.2.attn.out_proj.weight\", \"visual.transformer.resblocks.2.attn.out_proj.bias\", \"visual.transformer.resblocks.2.ln_2.weight\", \"visual.transformer.resblocks.2.ln_2.bias\", \"visual.transformer.resblocks.2.mlp.c_fc.weight\", \"visual.transformer.resblocks.2.mlp.c_fc.bias\", \"visual.transformer.resblocks.2.mlp.c_proj.weight\", \"visual.transformer.resblocks.2.mlp.c_proj.bias\", \"visual.transformer.resblocks.3.ln_1.weight\", \"visual.transformer.resblocks.3.ln_1.bias\", \"visual.transformer.resblocks.3.attn.in_proj_weight\", \"visual.transformer.resblocks.3.attn.in_proj_bias\", \"visual.transformer.resblocks.3.attn.out_proj.weight\", \"visual.transformer.resblocks.3.attn.out_proj.bias\", \"visual.transformer.resblocks.3.ln_2.weight\", \"visual.transformer.resblocks.3.ln_2.bias\", \"visual.transformer.resblocks.3.mlp.c_fc.weight\", \"visual.transformer.resblocks.3.mlp.c_fc.bias\", \"visual.transformer.resblocks.3.mlp.c_proj.weight\", \"visual.transformer.resblocks.3.mlp.c_proj.bias\", \"visual.transformer.resblocks.4.ln_1.weight\", \"visual.transformer.resblocks.4.ln_1.bias\", \"visual.transformer.resblocks.4.attn.in_proj_weight\", \"visual.transformer.resblocks.4.attn.in_proj_bias\", \"visual.transformer.resblocks.4.attn.out_proj.weight\", \"visual.transformer.resblocks.4.attn.out_proj.bias\", \"visual.transformer.resblocks.4.ln_2.weight\", \"visual.transformer.resblocks.4.ln_2.bias\", \"visual.transformer.resblocks.4.mlp.c_fc.weight\", \"visual.transformer.resblocks.4.mlp.c_fc.bias\", \"visual.transformer.resblocks.4.mlp.c_proj.weight\", \"visual.transformer.resblocks.4.mlp.c_proj.bias\", \"visual.transformer.resblocks.5.ln_1.weight\", \"visual.transformer.resblocks.5.ln_1.bias\", \"visual.transformer.resblocks.5.attn.in_proj_weight\", \"visual.transformer.resblocks.5.attn.in_proj_bias\", \"visual.transformer.resblocks.5.attn.out_proj.weight\", \"visual.transformer.resblocks.5.attn.out_proj.bias\", \"visual.transformer.resblocks.5.ln_2.weight\", \"visual.transformer.resblocks.5.ln_2.bias\", \"visual.transformer.resblocks.5.mlp.c_fc.weight\", \"visual.transformer.resblocks.5.mlp.c_fc.bias\", \"visual.transformer.resblocks.5.mlp.c_proj.weight\", \"visual.transformer.resblocks.5.mlp.c_proj.bias\", \"visual.transformer.resblocks.6.ln_1.weight\", \"visual.transformer.resblocks.6.ln_1.bias\", \"visual.transformer.resblocks.6.attn.in_proj_weight\", \"visual.transformer.resblocks.6.attn.in_proj_bias\", \"visual.transformer.resblocks.6.attn.out_proj.weight\", \"visual.transformer.resblocks.6.attn.out_proj.bias\", \"visual.transformer.resblocks.6.ln_2.weight\", \"visual.transformer.resblocks.6.ln_2.bias\", \"visual.transformer.resblocks.6.mlp.c_fc.weight\", \"visual.transformer.resblocks.6.mlp.c_fc.bias\", \"visual.transformer.resblocks.6.mlp.c_proj.weight\", \"visual.transformer.resblocks.6.mlp.c_proj.bias\", \"visual.transformer.resblocks.7.ln_1.weight\", \"visual.transformer.resblocks.7.ln_1.bias\", \"visual.transformer.resblocks.7.attn.in_proj_weight\", \"visual.transformer.resblocks.7.attn.in_proj_bias\", \"visual.transformer.resblocks.7.attn.out_proj.weight\", \"visual.transformer.resblocks.7.attn.out_proj.bias\", \"visual.transformer.resblocks.7.ln_2.weight\", \"visual.transformer.resblocks.7.ln_2.bias\", \"visual.transformer.resblocks.7.mlp.c_fc.weight\", \"visual.transformer.resblocks.7.mlp.c_fc.bias\", \"visual.transformer.resblocks.7.mlp.c_proj.weight\", \"visual.transformer.resblocks.7.mlp.c_proj.bias\", \"visual.transformer.resblocks.8.ln_1.weight\", \"visual.transformer.resblocks.8.ln_1.bias\", \"visual.transformer.resblocks.8.attn.in_proj_weight\", \"visual.transformer.resblocks.8.attn.in_proj_bias\", \"visual.transformer.resblocks.8.attn.out_proj.weight\", \"visual.transformer.resblocks.8.attn.out_proj.bias\", \"visual.transformer.resblocks.8.ln_2.weight\", \"visual.transformer.resblocks.8.ln_2.bias\", \"visual.transformer.resblocks.8.mlp.c_fc.weight\", \"visual.transformer.resblocks.8.mlp.c_fc.bias\", \"visual.transformer.resblocks.8.mlp.c_proj.weight\", \"visual.transformer.resblocks.8.mlp.c_proj.bias\", \"visual.transformer.resblocks.9.ln_1.weight\", \"visual.transformer.resblocks.9.ln_1.bias\", \"visual.transformer.resblocks.9.attn.in_proj_weight\", \"visual.transformer.resblocks.9.attn.in_proj_bias\", \"visual.transformer.resblocks.9.attn.out_proj.weight\", \"visual.transformer.resblocks.9.attn.out_proj.bias\", \"visual.transformer.resblocks.9.ln_2.weight\", \"visual.transformer.resblocks.9.ln_2.bias\", \"visual.transformer.resblocks.9.mlp.c_fc.weight\", \"visual.transformer.resblocks.9.mlp.c_fc.bias\", \"visual.transformer.resblocks.9.mlp.c_proj.weight\", \"visual.transformer.resblocks.9.mlp.c_proj.bias\", \"visual.transformer.resblocks.10.ln_1.weight\", \"visual.transformer.resblocks.10.ln_1.bias\", \"visual.transformer.resblocks.10.attn.in_proj_weight\", \"visual.transformer.resblocks.10.attn.in_proj_bias\", \"visual.transformer.resblocks.10.attn.out_proj.weight\", \"visual.transformer.resblocks.10.attn.out_proj.bias\", \"visual.transformer.resblocks.10.ln_2.weight\", \"visual.transformer.resblocks.10.ln_2.bias\", \"visual.transformer.resblocks.10.mlp.c_fc.weight\", \"visual.transformer.resblocks.10.mlp.c_fc.bias\", \"visual.transformer.resblocks.10.mlp.c_proj.weight\", \"visual.transformer.resblocks.10.mlp.c_proj.bias\", \"visual.transformer.resblocks.11.ln_1.weight\", \"visual.transformer.resblocks.11.ln_1.bias\", \"visual.transformer.resblocks.11.attn.in_proj_weight\", \"visual.transformer.resblocks.11.attn.in_proj_bias\", \"visual.transformer.resblocks.11.attn.out_proj.weight\", \"visual.transformer.resblocks.11.attn.out_proj.bias\", \"visual.transformer.resblocks.11.ln_2.weight\", \"visual.transformer.resblocks.11.ln_2.bias\", \"visual.transformer.resblocks.11.mlp.c_fc.weight\", \"visual.transformer.resblocks.11.mlp.c_fc.bias\", \"visual.transformer.resblocks.11.mlp.c_proj.weight\", \"visual.transformer.resblocks.11.mlp.c_proj.bias\", \"visual.ln_post.weight\", \"visual.ln_post.bias\", \"transformer.resblocks.0.ln_1.weight\", \"transformer.resblocks.0.ln_1.bias\", \"transformer.resblocks.0.attn.in_proj_weight\", \"transformer.resblocks.0.attn.in_proj_bias\", \"transformer.resblocks.0.attn.out_proj.weight\", \"transformer.resblocks.0.attn.out_proj.bias\", \"transformer.resblocks.0.ln_2.weight\", \"transformer.resblocks.0.ln_2.bias\", \"transformer.resblocks.0.mlp.c_fc.weight\", \"transformer.resblocks.0.mlp.c_fc.bias\", \"transformer.resblocks.0.mlp.c_proj.weight\", \"transformer.resblocks.0.mlp.c_proj.bias\", \"transformer.resblocks.1.ln_1.weight\", \"transformer.resblocks.1.ln_1.bias\", \"transformer.resblocks.1.attn.in_proj_weight\", \"transformer.resblocks.1.attn.in_proj_bias\", \"transformer.resblocks.1.attn.out_proj.weight\", \"transformer.resblocks.1.attn.out_proj.bias\", \"transformer.resblocks.1.ln_2.weight\", \"transformer.resblocks.1.ln_2.bias\", \"transformer.resblocks.1.mlp.c_fc.weight\", \"transformer.resblocks.1.mlp.c_fc.bias\", \"transformer.resblocks.1.mlp.c_proj.weight\", \"transformer.resblocks.1.mlp.c_proj.bias\", \"transformer.resblocks.2.ln_1.weight\", \"transformer.resblocks.2.ln_1.bias\", \"transformer.resblocks.2.attn.in_proj_weight\", \"transformer.resblocks.2.attn.in_proj_bias\", \"transformer.resblocks.2.attn.out_proj.weight\", \"transformer.resblocks.2.attn.out_proj.bias\", \"transformer.resblocks.2.ln_2.weight\", \"transformer.resblocks.2.ln_2.bias\", \"transformer.resblocks.2.mlp.c_fc.weight\", \"transformer.resblocks.2.mlp.c_fc.bias\", \"transformer.resblocks.2.mlp.c_proj.weight\", \"transformer.resblocks.2.mlp.c_proj.bias\", \"transformer.resblocks.3.ln_1.weight\", \"transformer.resblocks.3.ln_1.bias\", \"transformer.resblocks.3.attn.in_proj_weight\", \"transformer.resblocks.3.attn.in_proj_bias\", \"transformer.resblocks.3.attn.out_proj.weight\", \"transformer.resblocks.3.attn.out_proj.bias\", \"transformer.resblocks.3.ln_2.weight\", \"transformer.resblocks.3.ln_2.bias\", \"transformer.resblocks.3.mlp.c_fc.weight\", \"transformer.resblocks.3.mlp.c_fc.bias\", \"transformer.resblocks.3.mlp.c_proj.weight\", \"transformer.resblocks.3.mlp.c_proj.bias\", \"transformer.resblocks.4.ln_1.weight\", \"transformer.resblocks.4.ln_1.bias\", \"transformer.resblocks.4.attn.in_proj_weight\", \"transformer.resblocks.4.attn.in_proj_bias\", \"transformer.resblocks.4.attn.out_proj.weight\", \"transformer.resblocks.4.attn.out_proj.bias\", \"transformer.resblocks.4.ln_2.weight\", \"transformer.resblocks.4.ln_2.bias\", \"transformer.resblocks.4.mlp.c_fc.weight\", \"transformer.resblocks.4.mlp.c_fc.bias\", \"transformer.resblocks.4.mlp.c_proj.weight\", \"transformer.resblocks.4.mlp.c_proj.bias\", \"transformer.resblocks.5.ln_1.weight\", \"transformer.resblocks.5.ln_1.bias\", \"transformer.resblocks.5.attn.in_proj_weight\", \"transformer.resblocks.5.attn.in_proj_bias\", \"transformer.resblocks.5.attn.out_proj.weight\", \"transformer.resblocks.5.attn.out_proj.bias\", \"transformer.resblocks.5.ln_2.weight\", \"transformer.resblocks.5.ln_2.bias\", \"transformer.resblocks.5.mlp.c_fc.weight\", \"transformer.resblocks.5.mlp.c_fc.bias\", \"transformer.resblocks.5.mlp.c_proj.weight\", \"transformer.resblocks.5.mlp.c_proj.bias\", \"transformer.resblocks.6.ln_1.weight\", \"transformer.resblocks.6.ln_1.bias\", \"transformer.resblocks.6.attn.in_proj_weight\", \"transformer.resblocks.6.attn.in_proj_bias\", \"transformer.resblocks.6.attn.out_proj.weight\", \"transformer.resblocks.6.attn.out_proj.bias\", \"transformer.resblocks.6.ln_2.weight\", \"transformer.resblocks.6.ln_2.bias\", \"transformer.resblocks.6.mlp.c_fc.weight\", \"transformer.resblocks.6.mlp.c_fc.bias\", \"transformer.resblocks.6.mlp.c_proj.weight\", \"transformer.resblocks.6.mlp.c_proj.bias\", \"transformer.resblocks.7.ln_1.weight\", \"transformer.resblocks.7.ln_1.bias\", \"transformer.resblocks.7.attn.in_proj_weight\", \"transformer.resblocks.7.attn.in_proj_bias\", \"transformer.resblocks.7.attn.out_proj.weight\", \"transformer.resblocks.7.attn.out_proj.bias\", \"transformer.resblocks.7.ln_2.weight\", \"transformer.resblocks.7.ln_2.bias\", \"transformer.resblocks.7.mlp.c_fc.weight\", \"transformer.resblocks.7.mlp.c_fc.bias\", \"transformer.resblocks.7.mlp.c_proj.weight\", \"transformer.resblocks.7.mlp.c_proj.bias\", \"transformer.resblocks.8.ln_1.weight\", \"transformer.resblocks.8.ln_1.bias\", \"transformer.resblocks.8.attn.in_proj_weight\", \"transformer.resblocks.8.attn.in_proj_bias\", \"transformer.resblocks.8.attn.out_proj.weight\", \"transformer.resblocks.8.attn.out_proj.bias\", \"transformer.resblocks.8.ln_2.weight\", \"transformer.resblocks.8.ln_2.bias\", \"transformer.resblocks.8.mlp.c_fc.weight\", \"transformer.resblocks.8.mlp.c_fc.bias\", \"transformer.resblocks.8.mlp.c_proj.weight\", \"transformer.resblocks.8.mlp.c_proj.bias\", \"transformer.resblocks.9.ln_1.weight\", \"transformer.resblocks.9.ln_1.bias\", \"transformer.resblocks.9.attn.in_proj_weight\", \"transformer.resblocks.9.attn.in_proj_bias\", \"transformer.resblocks.9.attn.out_proj.weight\", \"transformer.resblocks.9.attn.out_proj.bias\", \"transformer.resblocks.9.ln_2.weight\", \"transformer.resblocks.9.ln_2.bias\", \"transformer.resblocks.9.mlp.c_fc.weight\", \"transformer.resblocks.9.mlp.c_fc.bias\", \"transformer.resblocks.9.mlp.c_proj.weight\", \"transformer.resblocks.9.mlp.c_proj.bias\", \"transformer.resblocks.10.ln_1.weight\", \"transformer.resblocks.10.ln_1.bias\", \"transformer.resblocks.10.attn.in_proj_weight\", \"transformer.resblocks.10.attn.in_proj_bias\", \"transformer.resblocks.10.attn.out_proj.weight\", \"transformer.resblocks.10.attn.out_proj.bias\", \"transformer.resblocks.10.ln_2.weight\", \"transformer.resblocks.10.ln_2.bias\", \"transformer.resblocks.10.mlp.c_fc.weight\", \"transformer.resblocks.10.mlp.c_fc.bias\", \"transformer.resblocks.10.mlp.c_proj.weight\", \"transformer.resblocks.10.mlp.c_proj.bias\", \"transformer.resblocks.11.ln_1.weight\", \"transformer.resblocks.11.ln_1.bias\", \"transformer.resblocks.11.attn.in_proj_weight\", \"transformer.resblocks.11.attn.in_proj_bias\", \"transformer.resblocks.11.attn.out_proj.weight\", \"transformer.resblocks.11.attn.out_proj.bias\", \"transformer.resblocks.11.ln_2.weight\", \"transformer.resblocks.11.ln_2.bias\", \"transformer.resblocks.11.mlp.c_fc.weight\", \"transformer.resblocks.11.mlp.c_fc.bias\", \"transformer.resblocks.11.mlp.c_proj.weight\", \"transformer.resblocks.11.mlp.c_proj.bias\", \"token_embedding.weight\", \"ln_final.weight\", \"ln_final.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VisionTransformer:\n\tMissing key(s) in state_dict: \"cls_token\", \"pos_embed\", \"patch_embed.proj.weight\", \"patch_embed.proj.bias\", \"blocks.0.norm1.weight\", \"blocks.0.norm1.bias\", \"blocks.0.attn.qkv.weight\", \"blocks.0.attn.qkv.bias\", \"blocks.0.attn.proj.weight\", \"blocks.0.attn.proj.bias\", \"blocks.0.norm2.weight\", \"blocks.0.norm2.bias\", \"blocks.0.mlp.fc1.weight\", \"blocks.0.mlp.fc1.bias\", \"blocks.0.mlp.fc2.weight\", \"blocks.0.mlp.fc2.bias\", \"blocks.1.norm1.weight\", \"blocks.1.norm1.bias\", \"blocks.1.attn.qkv.weight\", \"blocks.1.attn.qkv.bias\", \"blocks.1.attn.proj.weight\", \"blocks.1.attn.proj.bias\", \"blocks.1.norm2.weight\", \"blocks.1.norm2.bias\", \"blocks.1.mlp.fc1.weight\", \"blocks.1.mlp.fc1.bias\", \"blocks.1.mlp.fc2.weight\", \"blocks.1.mlp.fc2.bias\", \"blocks.2.norm1.weight\", \"blocks.2.norm1.bias\", \"blocks.2.attn.qkv.weight\", \"blocks.2.attn.qkv.bias\", \"blocks.2.attn.proj.weight\", \"blocks.2.attn.proj.bias\", \"blocks.2.norm2.weight\", \"blocks.2.norm2.bias\", \"blocks.2.mlp.fc1.weight\", \"blocks.2.mlp.fc1.bias\", \"blocks.2.mlp.fc2.weight\", \"blocks.2.mlp.fc2.bias\", \"blocks.3.norm1.weight\", \"blocks.3.norm1.bias\", \"blocks.3.attn.qkv.weight\", \"blocks.3.attn.qkv.bias\", \"blocks.3.attn.proj.weight\", \"blocks.3.attn.proj.bias\", \"blocks.3.norm2.weight\", \"blocks.3.norm2.bias\", \"blocks.3.mlp.fc1.weight\", \"blocks.3.mlp.fc1.bias\", \"blocks.3.mlp.fc2.weight\", \"blocks.3.mlp.fc2.bias\", \"blocks.4.norm1.weight\", \"blocks.4.norm1.bias\", \"blocks.4.attn.qkv.weight\", \"blocks.4.attn.qkv.bias\", \"blocks.4.attn.proj.weight\", \"blocks.4.attn.proj.bias\", \"blocks.4.norm2.weight\", \"blocks.4.norm2.bias\", \"blocks.4.mlp.fc1.weight\", \"blocks.4.mlp.fc1.bias\", \"blocks.4.mlp.fc2.weight\", \"blocks.4.mlp.fc2.bias\", \"blocks.5.norm1.weight\", \"blocks.5.norm1.bias\", \"blocks.5.attn.qkv.weight\", \"blocks.5.attn.qkv.bias\", \"blocks.5.attn.proj.weight\", \"blocks.5.attn.proj.bias\", \"blocks.5.norm2.weight\", \"blocks.5.norm2.bias\", \"blocks.5.mlp.fc1.weight\", \"blocks.5.mlp.fc1.bias\", \"blocks.5.mlp.fc2.weight\", \"blocks.5.mlp.fc2.bias\", \"blocks.6.norm1.weight\", \"blocks.6.norm1.bias\", \"blocks.6.attn.qkv.weight\", \"blocks.6.attn.qkv.bias\", \"blocks.6.attn.proj.weight\", \"blocks.6.attn.proj.bias\", \"blocks.6.norm2.weight\", \"blocks.6.norm2.bias\", \"blocks.6.mlp.fc1.weight\", \"blocks.6.mlp.fc1.bias\", \"blocks.6.mlp.fc2.weight\", \"blocks.6.mlp.fc2.bias\", \"blocks.7.norm1.weight\", \"blocks.7.norm1.bias\", \"blocks.7.attn.qkv.weight\", \"blocks.7.attn.qkv.bias\", \"blocks.7.attn.proj.weight\", \"blocks.7.attn.proj.bias\", \"blocks.7.norm2.weight\", \"blocks.7.norm2.bias\", \"blocks.7.mlp.fc1.weight\", \"blocks.7.mlp.fc1.bias\", \"blocks.7.mlp.fc2.weight\", \"blocks.7.mlp.fc2.bias\", \"blocks.8.norm1.weight\", \"blocks.8.norm1.bias\", \"blocks.8.attn.qkv.weight\", \"blocks.8.attn.qkv.bias\", \"blocks.8.attn.proj.weight\", \"blocks.8.attn.proj.bias\", \"blocks.8.norm2.weight\", \"blocks.8.norm2.bias\", \"blocks.8.mlp.fc1.weight\", \"blocks.8.mlp.fc1.bias\", \"blocks.8.mlp.fc2.weight\", \"blocks.8.mlp.fc2.bias\", \"blocks.9.norm1.weight\", \"blocks.9.norm1.bias\", \"blocks.9.attn.qkv.weight\", \"blocks.9.attn.qkv.bias\", \"blocks.9.attn.proj.weight\", \"blocks.9.attn.proj.bias\", \"blocks.9.norm2.weight\", \"blocks.9.norm2.bias\", \"blocks.9.mlp.fc1.weight\", \"blocks.9.mlp.fc1.bias\", \"blocks.9.mlp.fc2.weight\", \"blocks.9.mlp.fc2.bias\", \"blocks.10.norm1.weight\", \"blocks.10.norm1.bias\", \"blocks.10.attn.qkv.weight\", \"blocks.10.attn.qkv.bias\", \"blocks.10.attn.proj.weight\", \"blocks.10.attn.proj.bias\", \"blocks.10.norm2.weight\", \"blocks.10.norm2.bias\", \"blocks.10.mlp.fc1.weight\", \"blocks.10.mlp.fc1.bias\", \"blocks.10.mlp.fc2.weight\", \"blocks.10.mlp.fc2.bias\", \"blocks.11.norm1.weight\", \"blocks.11.norm1.bias\", \"blocks.11.attn.qkv.weight\", \"blocks.11.attn.qkv.bias\", \"blocks.11.attn.proj.weight\", \"blocks.11.attn.proj.bias\", \"blocks.11.norm2.weight\", \"blocks.11.norm2.bias\", \"blocks.11.mlp.fc1.weight\", \"blocks.11.mlp.fc1.bias\", \"blocks.11.mlp.fc2.weight\", \"blocks.11.mlp.fc2.bias\", \"norm.weight\", \"norm.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"positional_embedding\", \"text_projection\", \"logit_scale\", \"visual.class_embedding\", \"visual.positional_embedding\", \"visual.proj\", \"visual.conv1.weight\", \"visual.ln_pre.weight\", \"visual.ln_pre.bias\", \"visual.transformer.resblocks.0.ln_1.weight\", \"visual.transformer.resblocks.0.ln_1.bias\", \"visual.transformer.resblocks.0.attn.in_proj_weight\", \"visual.transformer.resblocks.0.attn.in_proj_bias\", \"visual.transformer.resblocks.0.attn.out_proj.weight\", \"visual.transformer.resblocks.0.attn.out_proj.bias\", \"visual.transformer.resblocks.0.ln_2.weight\", \"visual.transformer.resblocks.0.ln_2.bias\", \"visual.transformer.resblocks.0.mlp.c_fc.weight\", \"visual.transformer.resblocks.0.mlp.c_fc.bias\", \"visual.transformer.resblocks.0.mlp.c_proj.weight\", \"visual.transformer.resblocks.0.mlp.c_proj.bias\", \"visual.transformer.resblocks.1.ln_1.weight\", \"visual.transformer.resblocks.1.ln_1.bias\", \"visual.transformer.resblocks.1.attn.in_proj_weight\", \"visual.transformer.resblocks.1.attn.in_proj_bias\", \"visual.transformer.resblocks.1.attn.out_proj.weight\", \"visual.transformer.resblocks.1.attn.out_proj.bias\", \"visual.transformer.resblocks.1.ln_2.weight\", \"visual.transformer.resblocks.1.ln_2.bias\", \"visual.transformer.resblocks.1.mlp.c_fc.weight\", \"visual.transformer.resblocks.1.mlp.c_fc.bias\", \"visual.transformer.resblocks.1.mlp.c_proj.weight\", \"visual.transformer.resblocks.1.mlp.c_proj.bias\", \"visual.transformer.resblocks.2.ln_1.weight\", \"visual.transformer.resblocks.2.ln_1.bias\", \"visual.transformer.resblocks.2.attn.in_proj_weight\", \"visual.transformer.resblocks.2.attn.in_proj_bias\", \"visual.transformer.resblocks.2.attn.out_proj.weight\", \"visual.transformer.resblocks.2.attn.out_proj.bias\", \"visual.transformer.resblocks.2.ln_2.weight\", \"visual.transformer.resblocks.2.ln_2.bias\", \"visual.transformer.resblocks.2.mlp.c_fc.weight\", \"visual.transformer.resblocks.2.mlp.c_fc.bias\", \"visual.transformer.resblocks.2.mlp.c_proj.weight\", \"visual.transformer.resblocks.2.mlp.c_proj.bias\", \"visual.transformer.resblocks.3.ln_1.weight\", \"visual.transformer.resblocks.3.ln_1.bias\", \"visual.transformer.resblocks.3.attn.in_proj_weight\", \"visual.transformer.resblocks.3.attn.in_proj_bias\", \"visual.transformer.resblocks.3.attn.out_proj.weight\", \"visual.transformer.resblocks.3.attn.out_proj.bias\", \"visual.transformer.resblocks.3.ln_2.weight\", \"visual.transformer.resblocks.3.ln_2.bias\", \"visual.transformer.resblocks.3.mlp.c_fc.weight\", \"visual.transformer.resblocks.3.mlp.c_fc.bias\", \"visual.transformer.resblocks.3.mlp.c_proj.weight\", \"visual.transformer.resblocks.3.mlp.c_proj.bias\", \"visual.transformer.resblocks.4.ln_1.weight\", \"visual.transformer.resblocks.4.ln_1.bias\", \"visual.transformer.resblocks.4.attn.in_proj_weight\", \"visual.transformer.resblocks.4.attn.in_proj_bias\", \"visual.transformer.resblocks.4.attn.out_proj.weight\", \"visual.transformer.resblocks.4.attn.out_proj.bias\", \"visual.transformer.resblocks.4.ln_2.weight\", \"visual.transformer.resblocks.4.ln_2.bias\", \"visual.transformer.resblocks.4.mlp.c_fc.weight\", \"visual.transformer.resblocks.4.mlp.c_fc.bias\", \"visual.transformer.resblocks.4.mlp.c_proj.weight\", \"visual.transformer.resblocks.4.mlp.c_proj.bias\", \"visual.transformer.resblocks.5.ln_1.weight\", \"visual.transformer.resblocks.5.ln_1.bias\", \"visual.transformer.resblocks.5.attn.in_proj_weight\", \"visual.transformer.resblocks.5.attn.in_proj_bias\", \"visual.transformer.resblocks.5.attn.out_proj.weight\", \"visual.transformer.resblocks.5.attn.out_proj.bias\", \"visual.transformer.resblocks.5.ln_2.weight\", \"visual.transformer.resblocks.5.ln_2.bias\", \"visual.transformer.resblocks.5.mlp.c_fc.weight\", \"visual.transformer.resblocks.5.mlp.c_fc.bias\", \"visual.transformer.resblocks.5.mlp.c_proj.weight\", \"visual.transformer.resblocks.5.mlp.c_proj.bias\", \"visual.transformer.resblocks.6.ln_1.weight\", \"visual.transformer.resblocks.6.ln_1.bias\", \"visual.transformer.resblocks.6.attn.in_proj_weight\", \"visual.transformer.resblocks.6.attn.in_proj_bias\", \"visual.transformer.resblocks.6.attn.out_proj.weight\", \"visual.transformer.resblocks.6.attn.out_proj.bias\", \"visual.transformer.resblocks.6.ln_2.weight\", \"visual.transformer.resblocks.6.ln_2.bias\", \"visual.transformer.resblocks.6.mlp.c_fc.weight\", \"visual.transformer.resblocks.6.mlp.c_fc.bias\", \"visual.transformer.resblocks.6.mlp.c_proj.weight\", \"visual.transformer.resblocks.6.mlp.c_proj.bias\", \"visual.transformer.resblocks.7.ln_1.weight\", \"visual.transformer.resblocks.7.ln_1.bias\", \"visual.transformer.resblocks.7.attn.in_proj_weight\", \"visual.transformer.resblocks.7.attn.in_proj_bias\", \"visual.transformer.resblocks.7.attn.out_proj.weight\", \"visual.transformer.resblocks.7.attn.out_proj.bias\", \"visual.transformer.resblocks.7.ln_2.weight\", \"visual.transformer.resblocks.7.ln_2.bias\", \"visual.transformer.resblocks.7.mlp.c_fc.weight\", \"visual.transformer.resblocks.7.mlp.c_fc.bias\", \"visual.transformer.resblocks.7.mlp.c_proj.weight\", \"visual.transformer.resblocks.7.mlp.c_proj.bias\", \"visual.transformer.resblocks.8.ln_1.weight\", \"visual.transformer.resblocks.8.ln_1.bias\", \"visual.transformer.resblocks.8.attn.in_proj_weight\", \"visual.transformer.resblocks.8.attn.in_proj_bias\", \"visual.transformer.resblocks.8.attn.out_proj.weight\", \"visual.transformer.resblocks.8.attn.out_proj.bias\", \"visual.transformer.resblocks.8.ln_2.weight\", \"visual.transformer.resblocks.8.ln_2.bias\", \"visual.transformer.resblocks.8.mlp.c_fc.weight\", \"visual.transformer.resblocks.8.mlp.c_fc.bias\", \"visual.transformer.resblocks.8.mlp.c_proj.weight\", \"visual.transformer.resblocks.8.mlp.c_proj.bias\", \"visual.transformer.resblocks.9.ln_1.weight\", \"visual.transformer.resblocks.9.ln_1.bias\", \"visual.transformer.resblocks.9.attn.in_proj_weight\", \"visual.transformer.resblocks.9.attn.in_proj_bias\", \"visual.transformer.resblocks.9.attn.out_proj.weight\", \"visual.transformer.resblocks.9.attn.out_proj.bias\", \"visual.transformer.resblocks.9.ln_2.weight\", \"visual.transformer.resblocks.9.ln_2.bias\", \"visual.transformer.resblocks.9.mlp.c_fc.weight\", \"visual.transformer.resblocks.9.mlp.c_fc.bias\", \"visual.transformer.resblocks.9.mlp.c_proj.weight\", \"visual.transformer.resblocks.9.mlp.c_proj.bias\", \"visual.transformer.resblocks.10.ln_1.weight\", \"visual.transformer.resblocks.10.ln_1.bias\", \"visual.transformer.resblocks.10.attn.in_proj_weight\", \"visual.transformer.resblocks.10.attn.in_proj_bias\", \"visual.transformer.resblocks.10.attn.out_proj.weight\", \"visual.transformer.resblocks.10.attn.out_proj.bias\", \"visual.transformer.resblocks.10.ln_2.weight\", \"visual.transformer.resblocks.10.ln_2.bias\", \"visual.transformer.resblocks.10.mlp.c_fc.weight\", \"visual.transformer.resblocks.10.mlp.c_fc.bias\", \"visual.transformer.resblocks.10.mlp.c_proj.weight\", \"visual.transformer.resblocks.10.mlp.c_proj.bias\", \"visual.transformer.resblocks.11.ln_1.weight\", \"visual.transformer.resblocks.11.ln_1.bias\", \"visual.transformer.resblocks.11.attn.in_proj_weight\", \"visual.transformer.resblocks.11.attn.in_proj_bias\", \"visual.transformer.resblocks.11.attn.out_proj.weight\", \"visual.transformer.resblocks.11.attn.out_proj.bias\", \"visual.transformer.resblocks.11.ln_2.weight\", \"visual.transformer.resblocks.11.ln_2.bias\", \"visual.transformer.resblocks.11.mlp.c_fc.weight\", \"visual.transformer.resblocks.11.mlp.c_fc.bias\", \"visual.transformer.resblocks.11.mlp.c_proj.weight\", \"visual.transformer.resblocks.11.mlp.c_proj.bias\", \"visual.ln_post.weight\", \"visual.ln_post.bias\", \"transformer.resblocks.0.ln_1.weight\", \"transformer.resblocks.0.ln_1.bias\", \"transformer.resblocks.0.attn.in_proj_weight\", \"transformer.resblocks.0.attn.in_proj_bias\", \"transformer.resblocks.0.attn.out_proj.weight\", \"transformer.resblocks.0.attn.out_proj.bias\", \"transformer.resblocks.0.ln_2.weight\", \"transformer.resblocks.0.ln_2.bias\", \"transformer.resblocks.0.mlp.c_fc.weight\", \"transformer.resblocks.0.mlp.c_fc.bias\", \"transformer.resblocks.0.mlp.c_proj.weight\", \"transformer.resblocks.0.mlp.c_proj.bias\", \"transformer.resblocks.1.ln_1.weight\", \"transformer.resblocks.1.ln_1.bias\", \"transformer.resblocks.1.attn.in_proj_weight\", \"transformer.resblocks.1.attn.in_proj_bias\", \"transformer.resblocks.1.attn.out_proj.weight\", \"transformer.resblocks.1.attn.out_proj.bias\", \"transformer.resblocks.1.ln_2.weight\", \"transformer.resblocks.1.ln_2.bias\", \"transformer.resblocks.1.mlp.c_fc.weight\", \"transformer.resblocks.1.mlp.c_fc.bias\", \"transformer.resblocks.1.mlp.c_proj.weight\", \"transformer.resblocks.1.mlp.c_proj.bias\", \"transformer.resblocks.2.ln_1.weight\", \"transformer.resblocks.2.ln_1.bias\", \"transformer.resblocks.2.attn.in_proj_weight\", \"transformer.resblocks.2.attn.in_proj_bias\", \"transformer.resblocks.2.attn.out_proj.weight\", \"transformer.resblocks.2.attn.out_proj.bias\", \"transformer.resblocks.2.ln_2.weight\", \"transformer.resblocks.2.ln_2.bias\", \"transformer.resblocks.2.mlp.c_fc.weight\", \"transformer.resblocks.2.mlp.c_fc.bias\", \"transformer.resblocks.2.mlp.c_proj.weight\", \"transformer.resblocks.2.mlp.c_proj.bias\", \"transformer.resblocks.3.ln_1.weight\", \"transformer.resblocks.3.ln_1.bias\", \"transformer.resblocks.3.attn.in_proj_weight\", \"transformer.resblocks.3.attn.in_proj_bias\", \"transformer.resblocks.3.attn.out_proj.weight\", \"transformer.resblocks.3.attn.out_proj.bias\", \"transformer.resblocks.3.ln_2.weight\", \"transformer.resblocks.3.ln_2.bias\", \"transformer.resblocks.3.mlp.c_fc.weight\", \"transformer.resblocks.3.mlp.c_fc.bias\", \"transformer.resblocks.3.mlp.c_proj.weight\", \"transformer.resblocks.3.mlp.c_proj.bias\", \"transformer.resblocks.4.ln_1.weight\", \"transformer.resblocks.4.ln_1.bias\", \"transformer.resblocks.4.attn.in_proj_weight\", \"transformer.resblocks.4.attn.in_proj_bias\", \"transformer.resblocks.4.attn.out_proj.weight\", \"transformer.resblocks.4.attn.out_proj.bias\", \"transformer.resblocks.4.ln_2.weight\", \"transformer.resblocks.4.ln_2.bias\", \"transformer.resblocks.4.mlp.c_fc.weight\", \"transformer.resblocks.4.mlp.c_fc.bias\", \"transformer.resblocks.4.mlp.c_proj.weight\", \"transformer.resblocks.4.mlp.c_proj.bias\", \"transformer.resblocks.5.ln_1.weight\", \"transformer.resblocks.5.ln_1.bias\", \"transformer.resblocks.5.attn.in_proj_weight\", \"transformer.resblocks.5.attn.in_proj_bias\", \"transformer.resblocks.5.attn.out_proj.weight\", \"transformer.resblocks.5.attn.out_proj.bias\", \"transformer.resblocks.5.ln_2.weight\", \"transformer.resblocks.5.ln_2.bias\", \"transformer.resblocks.5.mlp.c_fc.weight\", \"transformer.resblocks.5.mlp.c_fc.bias\", \"transformer.resblocks.5.mlp.c_proj.weight\", \"transformer.resblocks.5.mlp.c_proj.bias\", \"transformer.resblocks.6.ln_1.weight\", \"transformer.resblocks.6.ln_1.bias\", \"transformer.resblocks.6.attn.in_proj_weight\", \"transformer.resblocks.6.attn.in_proj_bias\", \"transformer.resblocks.6.attn.out_proj.weight\", \"transformer.resblocks.6.attn.out_proj.bias\", \"transformer.resblocks.6.ln_2.weight\", \"transformer.resblocks.6.ln_2.bias\", \"transformer.resblocks.6.mlp.c_fc.weight\", \"transformer.resblocks.6.mlp.c_fc.bias\", \"transformer.resblocks.6.mlp.c_proj.weight\", \"transformer.resblocks.6.mlp.c_proj.bias\", \"transformer.resblocks.7.ln_1.weight\", \"transformer.resblocks.7.ln_1.bias\", \"transformer.resblocks.7.attn.in_proj_weight\", \"transformer.resblocks.7.attn.in_proj_bias\", \"transformer.resblocks.7.attn.out_proj.weight\", \"transformer.resblocks.7.attn.out_proj.bias\", \"transformer.resblocks.7.ln_2.weight\", \"transformer.resblocks.7.ln_2.bias\", \"transformer.resblocks.7.mlp.c_fc.weight\", \"transformer.resblocks.7.mlp.c_fc.bias\", \"transformer.resblocks.7.mlp.c_proj.weight\", \"transformer.resblocks.7.mlp.c_proj.bias\", \"transformer.resblocks.8.ln_1.weight\", \"transformer.resblocks.8.ln_1.bias\", \"transformer.resblocks.8.attn.in_proj_weight\", \"transformer.resblocks.8.attn.in_proj_bias\", \"transformer.resblocks.8.attn.out_proj.weight\", \"transformer.resblocks.8.attn.out_proj.bias\", \"transformer.resblocks.8.ln_2.weight\", \"transformer.resblocks.8.ln_2.bias\", \"transformer.resblocks.8.mlp.c_fc.weight\", \"transformer.resblocks.8.mlp.c_fc.bias\", \"transformer.resblocks.8.mlp.c_proj.weight\", \"transformer.resblocks.8.mlp.c_proj.bias\", \"transformer.resblocks.9.ln_1.weight\", \"transformer.resblocks.9.ln_1.bias\", \"transformer.resblocks.9.attn.in_proj_weight\", \"transformer.resblocks.9.attn.in_proj_bias\", \"transformer.resblocks.9.attn.out_proj.weight\", \"transformer.resblocks.9.attn.out_proj.bias\", \"transformer.resblocks.9.ln_2.weight\", \"transformer.resblocks.9.ln_2.bias\", \"transformer.resblocks.9.mlp.c_fc.weight\", \"transformer.resblocks.9.mlp.c_fc.bias\", \"transformer.resblocks.9.mlp.c_proj.weight\", \"transformer.resblocks.9.mlp.c_proj.bias\", \"transformer.resblocks.10.ln_1.weight\", \"transformer.resblocks.10.ln_1.bias\", \"transformer.resblocks.10.attn.in_proj_weight\", \"transformer.resblocks.10.attn.in_proj_bias\", \"transformer.resblocks.10.attn.out_proj.weight\", \"transformer.resblocks.10.attn.out_proj.bias\", \"transformer.resblocks.10.ln_2.weight\", \"transformer.resblocks.10.ln_2.bias\", \"transformer.resblocks.10.mlp.c_fc.weight\", \"transformer.resblocks.10.mlp.c_fc.bias\", \"transformer.resblocks.10.mlp.c_proj.weight\", \"transformer.resblocks.10.mlp.c_proj.bias\", \"transformer.resblocks.11.ln_1.weight\", \"transformer.resblocks.11.ln_1.bias\", \"transformer.resblocks.11.attn.in_proj_weight\", \"transformer.resblocks.11.attn.in_proj_bias\", \"transformer.resblocks.11.attn.out_proj.weight\", \"transformer.resblocks.11.attn.out_proj.bias\", \"transformer.resblocks.11.ln_2.weight\", \"transformer.resblocks.11.ln_2.bias\", \"transformer.resblocks.11.mlp.c_fc.weight\", \"transformer.resblocks.11.mlp.c_fc.bias\", \"transformer.resblocks.11.mlp.c_proj.weight\", \"transformer.resblocks.11.mlp.c_proj.bias\", \"token_embedding.weight\", \"ln_final.weight\", \"ln_final.bias\". "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionTransformer(\n",
       "  (embeddings): CLIPVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (position_embedding): Embedding(197, 768)\n",
       "  )\n",
       "  (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (encoder): CLIPEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CLIPVisionModelWithProjection were not initialized from the model checkpoint at /scratch/choi/output/Diff-Rep/clip/ and are newly initialized: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.pre_layrnorm.weight', 'visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# image_encoder_path = \"models/image_encoder/\"\n",
    "image_encoder_path = \"/scratch/choi/output/Diff-Rep/clip/\"\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "laion/CLIP-ViT-B-16-laion2B-s34B-b88K does not appear to have a file named config.json. Checkout 'https://huggingface.co/laion/CLIP-ViT-B-16-laion2B-s34B-b88K/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/laion/CLIP-ViT-B-16-laion2B-s34B-b88K/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 393\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6620ec4b-22f7f6d95cbee5a54a172665;d9980baa-f96b-4bdf-be15-e03a65e082b3)\n\nEntry Not Found for url: https://huggingface.co/laion/CLIP-ViT-B-16-laion2B-s34B-b88K/resolve/main/config.json.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCLIPVisionModelWithProjection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlaion/CLIP-ViT-B-16-laion2B-s34B-b88K\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/transformers/modeling_utils.py:3122\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3121\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 3122\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3138\u001b[0m     \u001b[38;5;66;03m# In case one passes a config to `from_pretrained` + \"attn_implementation\"\u001b[39;00m\n\u001b[1;32m   3139\u001b[0m     \u001b[38;5;66;03m# override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3143\u001b[0m     \u001b[38;5;66;03m# we pop attn_implementation from the kwargs but this handles the case where users\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m     \u001b[38;5;66;03m# passes manually the config to `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   3145\u001b[0m     config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/transformers/models/clip/configuration_clip.py:249\u001b[0m, in \u001b[0;36mCLIPVisionConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs)\n\u001b[0;32m--> 249\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# get the vision config dict if we are loading from CLIPConfig\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/transformers/utils/hub.py:452\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m         revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    457\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001b[0;31mOSError\u001b[0m: laion/CLIP-ViT-B-16-laion2B-s34B-b88K does not appear to have a file named config.json. Checkout 'https://huggingface.co/laion/CLIP-ViT-B-16-laion2B-s34B-b88K/main' for available files."
     ]
    }
   ],
   "source": [
    "CLIPVisionModelWithProjection.from_pretrained('laion/CLIP-ViT-B-16-laion2B-s34B-b88K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModelWithProjection(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CLIPVisionModel were not initialized from the model checkpoint at /scratch/choi/output/Diff-Rep/clip/ and are newly initialized: ['vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.pre_layrnorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLIPVisionModel.from_pretrained(image_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (1): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "CLIPVisionModelWithProjection_Custom = nn.Sequential(model.vision_model, model.visual_projection)\n",
    "CLIPVisionModelWithProjection_Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPVisionModelWithProjection, CLIPVisionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModelWithProjection(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"/scratch/choi/model/clip-vit-base-patch16\"\n",
    "model_proj = CLIPVisionModelWithProjection.from_pretrained(p)\n",
    "model_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CLIPVisionEmbeddings(\n",
       "   (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "   (position_embedding): Embedding(197, 768)\n",
       " ),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " CLIPEncoder(\n",
       "   (layers): ModuleList(\n",
       "     (0-11): 12 x CLIPEncoderLayer(\n",
       "       (self_attn): CLIPAttention(\n",
       "         (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "       )\n",
       "       (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): CLIPMLP(\n",
       "         (activation_fn): QuickGELUActivation()\n",
       "         (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "       )\n",
       "       (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " LayerNorm((768,), eps=1e-05, elementwise_affine=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(model_proj.children())[0].children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionTransformer(\n",
      "  (embeddings): CLIPVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "    (position_embedding): Embedding(197, 768)\n",
      "  )\n",
      "  (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder): CLIPEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x CLIPEncoderLayer(\n",
      "        (self_attn): CLIPAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): CLIPMLP(\n",
      "          (activation_fn): QuickGELUActivation()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "hi\n",
      "Linear(in_features=768, out_features=512, bias=False)\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "for i in list(model_proj.children())[:]:\n",
    "    print(i)\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CLIPVisionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPVisionModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(p)\n\u001b[1;32m      2\u001b[0m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CLIPVisionModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = CLIPVisionModel.from_pretrained(p)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(p)\n",
    "processor = AutoProcessor.from_pretrained(p)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "image_embeds = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = [i for i in [1, 2]]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43737494,  0.68529756, -0.47153484],\n",
       "       [ 0.78958552,  0.60254911, -0.27639124],\n",
       "       [ 0.8728852 ,  0.41399483,  1.9047565 ],\n",
       "       [ 0.59446415, -0.76260194,  1.2442162 ],\n",
       "       [ 0.32562513,  0.08326177,  1.09245435],\n",
       "       [-1.06670812,  0.16552778,  1.53316383]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randn(3, 2, 3)\n",
    "np.concatenate(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['image_embeds', 'last_hidden_state'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(p)\n",
    "processor = AutoProcessor.from_pretrained(p)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.head = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(768, affine=False, eps=1e-6),\n",
    "    torch.nn.Linear(768, 1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPooling' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maaa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPooling' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 768])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43maaa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooler_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/functional.py:2480\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2468\u001b[0m         batch_norm,\n\u001b[1;32m   2469\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2477\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2478\u001b[0m     )\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2480\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2484\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2446\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 768])"
     ]
    }
   ],
   "source": [
    "model.head(aaa.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0.00085**0.5, 0.012**0.5, 1000, dtype=torch.float32) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0d8265e870>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0d8265c230>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = torch.linspace(0.00085, 0.012, 1000, dtype=torch.float32)\n",
    "plt.plot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37, 88, 71, 64, 14],\n",
      "        [11, 31, 76, 25, 10],\n",
      "        [79, 69, 66, 97, 37]])\n",
      "tensor([[1369,  121, 6241],\n",
      "        [7744,  961, 4761],\n",
      "        [5041, 5776, 4356],\n",
      "        [4096,  625, 9409],\n",
      "        [ 196,  100, 1369]])\n",
      "tensor([[18446,  7583, 26136]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(100, (3, 5))\n",
    "print(a)\n",
    "b = a.T.pow(2)\n",
    "# b = a.T\n",
    "print(b)\n",
    "print(b.sum(dim=0, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8100,   36, 3721, 2401, 8836],\n",
      "        [8649,   25,   36, 1024, 1764],\n",
      "        [6889,  441,  900,  121,  784]])\n",
      "tensor([23094, 11498,  9135])\n",
      "tensor([[23094],\n",
      "        [11498],\n",
      "        [ 9135]])\n"
     ]
    }
   ],
   "source": [
    "c = a.pow(2)\n",
    "print(c)\n",
    "d = c.sum(dim=1)\n",
    "print(d)\n",
    "e = d[:, None]\n",
    "print(e)\n",
    "#  d_batch = X_batch.pow(2).sum(dim=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.argmin(dim=1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18, 92, 63, 55, 16],\n",
      "        [35, 68, 49, 64, 11],\n",
      "        [36, 37,  5, 76, 45]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2, 0, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.randint(100, (3, 5))\n",
    "print(f)\n",
    "f.argmin(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10, dtype=torch.long)\n",
    "b = torch.randn((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3162)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection, CLIPTokenizer\n",
    "\n",
    "text_model = CLIPTextModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "tokenizer2 = CLIPTokenizer.from_pretrained(\"/scratch/choi/model/stable-diffusion-v1-5\", subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer2([\"an image of a cat\", \"an image of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = text_model(**inputs)\n",
    "text_embeds = outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "vision_model = CLIPVisionModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "processor = AutoProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "cat_image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\"\n",
    "dog_image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model2 = CLIPVisionModelWithProjection.from_pretrained(\"/scratch/choi/model/IP-Adapter/models/image_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=[cat_image, dog_image], return_tensors=\"pt\")\n",
    "\n",
    "# outputs = vision_model(**inputs)\n",
    "outputs = vision_model2(**inputs)\n",
    "image_embeds = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2203, 0.0967],\n",
      "        [0.0863, 0.2006]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# normalized features\n",
    "image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# cosine similarity as logits -> (Batch_size, Batch_size)\n",
    "# logit_scale = self.logit_scale.exp()\n",
    "# logits_per_text = torch.matmul(text_embeds, image_embeds.t())*logit_scale\n",
    "logits_per_text = torch.matmul(text_embeds, image_embeds.t())\n",
    "print(logits_per_text)\n",
    "logits_per_image = logits_per_text.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n",
    "\n",
    "\n",
    "def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "    caption_loss = contrastive_loss(similarity)\n",
    "    image_loss = contrastive_loss(similarity.t())\n",
    "    return (caption_loss + image_loss) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6355, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_loss(logits_per_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPModel, CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "processor = AutoProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = CLIPImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model2 = CLIPVisionModelWithProjection.from_pretrained(\"/scratch/choi/model/IP-Adapter/models/image_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "raw_image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = transform(raw_image.convert(\"RGB\"))\n",
    "\n",
    "images = image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
    "# vision_model2(processor(images = image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = vision_model2(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['image_embeds', 'last_hidden_state'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "tensor(0.0139, grad_fn=<MeanBackward0>)\n",
      "tensor(5.9347, grad_fn=<MaxBackward1>)\n",
      "tensor(-5.4663, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(res2.image_embeds.shape)\n",
    "print(res2.image_embeds.mean())\n",
    "print(res2.image_embeds.max())\n",
    "print(res2.image_embeds.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = model.vision_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.visual_projection(res1.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "tensor(0.0139, grad_fn=<MeanBackward0>)\n",
      "tensor(5.9347, grad_fn=<MaxBackward1>)\n",
      "tensor(-5.4663, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)\n",
    "print(b.mean())\n",
    "print(b.max())\n",
    "print(b.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(4.6052, requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logit_scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-rep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
