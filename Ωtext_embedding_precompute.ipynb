{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text encoder pre-compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/Downloads/miniconda3/envs/diff-rep/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPVisionModelWithProjection\n",
    "\n",
    "import json\n",
    "import os\n",
    "from imagenet_classes import IMAGENET2012_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_path = \"/scratch/choi/model/stable-diffusion-v1-5\"\n",
    "text_encoder = CLIPTextModel.from_pretrained(sd_path, subfolder=\"text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(sd_path, subfolder=\"tokenizer\")\n",
    "# tokenizer2 = CLIPTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "# text_encoder = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "clip_vision_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.modeling_clip.CLIPVisionTransformer"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clip_model.vision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(next(clip_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(4.6052, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.605170249938965"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(clip_model.logit_scale.to(\"cuda:0\"))\n",
    "# clip_model.logit_scale.requires_grad_(True)\n",
    "print(clip_model.logit_scale)\n",
    "# clip_model.logit_scale.exp()\n",
    "clip_model.logit_scale.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(\n",
    "    open(\"/scratch/choi/dataset/ImageNet100/_img_text_pair_train.json\")\n",
    ")  # list of dict: [{\"image_file\": \"1.png\", \"text\": \"A dog\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "label_list = os.listdir(\"/scratch/choi/dataset/ImageNet100/train\")\n",
    "for label in label_list:\n",
    "    text_list.append(\"a photo of \" + IMAGENET2012_CLASSES[label])\n",
    "label_list.append(\"\")\n",
    "text_list.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = tokenizer(\n",
    "    text_list, padding=\"max_length\", truncation=True, return_tensors='pt'\n",
    ").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 77])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:2'\n",
    "clip_model = clip_model.vision_model.to(device)\n",
    "clip_vision_model = clip_vision_model.to(device)\n",
    "# text_encoder = text_encoder.to(device)\n",
    "# text_tokens = text_tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "processor = AutoProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = processor(images=image, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7675, -0.3751,  0.4466,  ...,  0.2628,  0.6986, -0.3035]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Parameter containing:\n",
      "tensor(4.6052, requires_grad=True)\n",
      "tensor(4.6052, device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i = i.to(\"cuda:0\")\n",
    "vis_model = clip_model.vision_model.to(\"cuda:0\")\n",
    "vis_proj = clip_model.visual_projection.to(\"cuda:0\")\n",
    "# clip_model = clip_model.to(\"cuda:1\")\n",
    "# clip_model.visual_projection(clip_model.vision_model(**i)[1])\n",
    "tmp = vis_model(**i)\n",
    "out = vis_proj(tmp[1])\n",
    "print(out)\n",
    "logit_scale = clip_model.logit_scale.to(\"cuda:0\")\n",
    "print(clip_model.logit_scale)\n",
    "print(logit_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = i.to('cuda:2')\n",
    "clip_vision_model = clip_vision_model.to(\"cuda:2\")\n",
    "out2 = clip_vision_model(**i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['image_embeds', 'last_hidden_state'])\n",
      "tensor([[-0.7675, -0.3751,  0.4466,  ...,  0.2628,  0.6986, -0.3035]],\n",
      "       device='cuda:2', grad_fn=<MmBackward0>)\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(out2.keys())\n",
    "print(out2[0])\n",
    "print(out2[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = text_encoder(text_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 77, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict(zip(label_list, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['n02114855'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a, \"IN100_text_embedding_dict_L.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out2 = clip_model.text_projection(clip_model.text_model(text_tokens)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dict(zip(label_list, out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['n02114855'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(b, 'IN100_text_embedding_dict_with_projection_H.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY LOADING DICT\n",
    "# -> NOTEBOOK TO SEPARATE FILE\n",
    "# -> APPLY DICT TO CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"IN100_text_embedding_dict_L.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['n02109047'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n",
       "        [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n",
       "        [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n",
       "        ...,\n",
       "        [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n",
       "        [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n",
       "        [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"IN100_text_embedding_dict_L.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-rep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
